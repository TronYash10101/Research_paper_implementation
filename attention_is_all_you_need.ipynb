{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"kjuoUeCIwTiY"},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6vV-V5RTwcAn"},"outputs":[],"source":["import tensorflow_datasets as tfds"]},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/indic-bert\")"],"metadata":{"id":"3c6rRCD9Yiva","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KmsO6uKMvzv7"},"source":["#Creating Embedding and positional embedding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6L6UyHcCUHhf"},"outputs":[],"source":["splits = {'train': 'data/train-00000-of-00001.parquet', 'validation': 'data/validation-00000-of-00001.parquet', 'test': 'data/test-00000-of-00001.parquet'}\n","df = pd.read_parquet(\"hf://datasets/cfilt/iitb-english-hindi/\" + splits[\"train\"])[:100_000]\n","df = np.array(df)"]},{"cell_type":"markdown","metadata":{"id":"mgFd6wxtyZ6G"},"source":["##Tokenizing Inputs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lzDMbD2S1fZM"},"outputs":[],"source":["hi_text = []\n","en_text = []\n","\n","max_len = 0\n","for i in range(10000):\n","  hi_text.append(df[i][0][\"hi\"])\n","  en_text.append(df[i][0][\"en\"])"]},{"cell_type":"code","source":["assert len(hi_text) == len(en_text), \"length words in input language does not match length of words in output language\"\n","\n","hi_token= tokenizer(hi_text,padding=True,return_tensors='tf')[\"input_ids\"]\n","en_token= tokenizer(en_text,padding=True,return_tensors='tf')[\"input_ids\"]"],"metadata":{"collapsed":true,"id":"QvQsUHkFjKDq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Creating Embedding of tokens"],"metadata":{"id":"2Hs4xrnooUOG"}},{"cell_type":"code","source":["vocab_size = tokenizer.vocab_size\n","\n","embedding_layer = tf.keras.layers.Embedding(input_dim=vocab_size + 1,output_dim=128)\n","hi_embedding = embedding_layer(hi_token)\n","en_embedding = embedding_layer(en_token)"],"metadata":{"id":"UOt-OUvZoX4D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["arr= [4,5,6,7]\n","arr[::3]"],"metadata":{"id":"PkisoByX9NNW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Positional encoding"],"metadata":{"id":"SQkXgKw_zAM0"}},{"cell_type":"code","source":["# pos_embed_arr = []\n","# for pos in range(58):\n","#       for i in range(128):\n","#         if i % 2 == 0:\n","#           pe = np.sin(pos/10000**(2*i/128))\n","#           pos_embed_arr.append(pe)\n","#         elif i % 2 != 0:\n","#           pe = np.cos(pos/10000**(2*i/128))\n","#           pos_embed_arr.append(pe)\n","\n","# pos_embed_arr = np.array(pos_embed_arr)\n","# pos_embed_arr = pos_embed_arr.reshape(58,128)\n","# pos_embed_arr"],"metadata":{"collapsed":true,"id":"UO5fFGwCe2RO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["d_model_encoder = en_embedding.shape[2]\n","d_model_decoder = hi_embedding.shape[2]\n","max_token_encoder = en_embedding.shape[1]\n","max_token_decoder = hi_embedding.shape[1]"],"metadata":{"id":"zcUr15hvy_zc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class PositionalEncoder(tf.keras.layers.Layer):\n","  def __init__(self,max_token,d_model,dtype = np.float32,**kwargs):\n","    super().__init__(dtype = dtype,**kwargs)\n","    self.d_model = d_model\n","    self.max_token = max_token\n","    assert d_model % 2 == 0, \"d_model should be even\"\n","\n","    self.pos_embed_arr = []\n","    for pos in range(self.max_token):\n","      for i in range(d_model):\n","        if i % 2 == 0:\n","          pe = np.sin(pos/10000**(2*i/self.d_model))\n","          self.pos_embed_arr.append(pe)\n","        elif i % 2 != 0:\n","          pe = np.cos(pos/10000**(2*i/self.d_model))\n","          self.pos_embed_arr.append(pe)\n","\n","    self.pos_embed_arr = np.array(self.pos_embed_arr)\n","    self.pos_embed_arr = self.pos_embed_arr.reshape(self.max_token,self.d_model)\n","\n","  def call(self,embedding_vector):\n","      return embedding_vector + self.pos_embed_arr"],"metadata":{"id":"8sLPE5Wi5DJL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pos_embedding_encoder = PositionalEncoder(max_token_encoder,d_model_encoder)\n","final_en_embedding = pos_embedding_encoder.call(en_embedding)\n","final_en_embedding"],"metadata":{"id":"aHruI_z3Kx82","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Add & Norm Layer"],"metadata":{"id":"DteDKwJOE5Ze"}},{"cell_type":"code","source":["class Add_Norm(tf.keras.layers.Layer):\n","  def __init__(self,**kwargs):\n","    super().__init__(dtype=np.float32,**kwargs)\n","    self.norm_layer = tf.keras.layers.LayerNormalization(axis=-1,epsilon=0.001)\n","\n","  def call(self,sublayer_output,residual_input):\n","    return self.norm_layer(sublayer_output + residual_input)"],"metadata":{"id":"KyX2zMhUE3mm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Multi-Head Attention / Scaled Dot Product Attention"],"metadata":{"id":"EyFffSlwhgKD"}},{"cell_type":"code","source":["class ScaledDotProduct_Attention(tf.keras.layers.Layer):\n","  def __init__(self,d_k:int,**kwargs):\n","    super().__init__(dtype=np.float32,**kwargs)\n","    self.d_k = d_k\n","\n","  def call(self,q,k,v):\n","    dot_product = tf.matmul(q,k,transpose_b=True) #score\n","    scaled_dot_product = dot_product / tf.sqrt(tf.cast(self.d_k,dtype=tf.float32))\n","    attention_weight = tf.nn.softmax(scaled_dot_product,axis = -1)\n","    output = tf.matmul(attention_weight,v)\n","    return output"],"metadata":{"id":"gl_IZdoSwQnE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MultiHead_Attention(tf.keras.layers.Layer):\n","  def __init__(self,d_model:int,heads:int,**kwargs):\n","    super().__init__(dtype=np.float32,**kwargs)\n","    self.d_model = d_model\n","    self.heads = heads\n","    assert d_model % heads == 0, \"d_model must be perfectly divisible by heads\"\n","    self.d_k = d_model // heads\n","    self.attention_fuction = ScaledDotProduct_Attention(self.d_k)\n","    self.wq = [tf.keras.layers.Dense(self.d_k,use_bias=False) for _ in range(self.heads)]\n","    self.wk = [tf.keras.layers.Dense(self.d_k,use_bias=False) for _ in range(self.heads)]\n","    self.wv = [tf.keras.layers.Dense(self.d_k,use_bias=False) for _ in range(self.heads)]\n","    self.final_linear_transform = tf.keras.layers.Dense(self.d_model,use_bias=False)\n","    self.add_norm_layer = Add_Norm()\n","\n","  def call(self,initial_embedding:tf.Tensor):\n","    concat_arr = []\n","    for head in range(self.heads):\n","      q = self.wq[head](initial_embedding)\n","      k = self.wk[head](initial_embedding)\n","      v = self.wv[head](initial_embedding)\n","\n","      output = self.attention_fuction.call(q,k,v)\n","      concat_arr.append(output)\n","    concat_output = tf.concat(concat_arr,axis=-1)\n","    mha_output = self.final_linear_transform(concat_output)\n","    residual_output = self.add_norm_layer.call(mha_output,initial_embedding)\n","    return residual_output"],"metadata":{"id":"W0ZimmjCDzVX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Feed Forward Network"],"metadata":{"id":"EakGMmlRNcNt"}},{"cell_type":"code","source":["class FFN(tf.keras.layers.Layer):\n","  def __init__(self,d_model:int,**kwargs):\n","    super().__init__(dtype=np.float32,**kwargs)\n","    self.d_model = d_model\n","    self.add_norm_layer = Add_Norm()\n","    self.network = tf.keras.Sequential([\n","        tf.keras.layers.Dense(self.d_model * 4,activation='relu'),\n","        tf.keras.layers.Dense(self.d_model)\n","    ])\n","\n","  def call(self,mha_output):\n","    nn_output = self.network(mha_output)\n","    ffn_output = self.add_norm_layer.call(nn_output,mha_output)\n","    return ffn_output"],"metadata":{"id":"VJn2e-U-As1M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#**Encoder**"],"metadata":{"id":"0r5AsHZ0NUCt"}},{"cell_type":"code","source":["class Encoder(tf.keras.layers.Layer):\n","  def __init__(self, d_model:int, heads:int,**kwargs):\n","    super().__init__(dtype=np.float32,**kwargs)\n","    self.d_model = d_model\n","    self.heads = heads\n","    self.mha = MultiHead_Attention(d_model=self.d_model,heads = self.heads)\n","    self.ffn = FFN(d_model=self.d_model)\n","\n","  def call(self,input_embedding):\n","    first_sublayer_out = self.mha.call(initial_embedding = input_embedding)\n","    ffn_output = self.ffn.call(mha_output = first_sublayer_out)\n","    encoder_output = ffn_output\n","    return encoder_output"],"metadata":{"id":"64rdrMpaNTpc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["encoder = Encoder(d_model=d_model_encoder,heads=4)\n","encoder.call(final_en_embedding)"],"metadata":{"id":"kaX2xhtdQBzz"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP+ts7BgijUxkfZwYudQkC1"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}